# ==========================================
# LiteLLM Proxy - Intelligent Virtual Router
# ==========================================
#
# ARCHITECTURE:
# 1. Virtual models (chat-auto, codex-auto, claude-auto) exist in model_list
#    → This passes LiteLLM's pre-hook model validation
# 2. async_pre_call_hook handles simple task routing (complexity-based)
# 3. LiteLLM router handles rate limit fallback (retry with next model in list)
#
# FALLBACK ORDER:
#   chat-auto   (3层): CLIProxyAPI → New API → Volces Ark API
#   claude-auto (2层): New API → Volces Ark API
#   codex-auto  (1层): New API only (Volces 不支持 Codex 接口)
#
# EXECUTION ORDER:
# Request → Virtual Key Auth → Model Alias Map → async_pre_call_hook (SIMPLE TASK CHECK) → Router (RATE LIMIT FALLBACK) → Backend APIs
# ==========================================

# ==========================================
# 2. ENVIRONMENT VARIABLES
# ==========================================
environment_variables:
  # 后端统一的 API Key (从 .env 加载)
  LITELLM_API_KEY: "${NEW_API_KEY}"
  OPENAI_API_KEY: "${NEW_API_KEY}"
  ANTHROPIC_API_KEY: "${NEW_API_KEY}"

  # Level 2: New API (第2层降级)
  NEW_API_BASE: "http://host.docker.internal:3000/v1"
  NEW_API_KEY: "${NEW_API_KEY}"

  # Level 1: CLIProxyAPI (第1层优先)
  CHAT_AUTO_API_BASE: "http://cliproxyapi:8317/v1"
  CHAT_AUTO_API_KEY: "${CHAT_AUTO_API_KEY}"

  # Level 3: Volces Ark API (第3层最终降级 - from .env)
  ARK_API_KEY: "${ARK_API_KEY}"
  ARK_OPENAI_BASE: "${ARK_OPENAI_BASE}"
  ARK_CLAUDE_BASE: "${ARK_CLAUDE_BASE}"

# ==========================================
# 3. MODEL DEFINITIONS
# ==========================================
model_list:
  # ==================================
  # VIRTUAL ENTRY POINTS
  # ==================================
  # These MUST exist to pass pre-hook validation

  - model_name: chat-auto
    litellm_params:
      model: "openai/gpt-5"
      api_base: http://cliproxyapi:8317/v1
      api_key: "${CHAT_AUTO_API_KEY}"
      custom_llm_provider: "openai"

  - model_name: chat-auto-mini
    litellm_params:
      model: "gpt-5-mini"
      api_base: http://cliproxyapi:8317/v1
      api_key: "${CHAT_AUTO_API_KEY}"
      custom_llm_provider: "openai"

  # codex-auto: 仅转发到 New API (无降级，Volces 不支持 Codex)
  - model_name: codex-auto
    litellm_params:
      model: "openai/gpt-5.2-codex"
      api_base: http://host.docker.internal:3000/v1
      api_key: "${NEW_API_KEY}"
      custom_llm_provider: "openai"

  # claude-auto: 第1层 New API (会降级到 Volces Ark)
  - model_name: claude-auto
    litellm_params:
      model: "anthropic/claude-sonnet-4-5"
      api_base: http://host.docker.internal:3000
      api_key: "${NEW_API_KEY}"
      custom_llm_provider: "anthropic"

  # ==================================
  # PHYSICAL MODELS - Chat Family
  # ==================================
  # 使用相同 model_name 创建 fallback 链

  # chat-auto 的 fallback 链 (3层降级)
  # Level 2: New API
  - model_name: chat-auto
    litellm_params:
      model: "gpt-5"
      api_base: http://host.docker.internal:3000/v1
      api_key: "${NEW_API_KEY}"
      custom_llm_provider: "openai"
    model_info:
      fallback_order: 2
      fallback_reason: "rate_limit"

  # Level 3: Volces Ark API (最终降级)
  - model_name: chat-auto
    litellm_params:
      model: "openai/glm-4.7"
      api_base: "${ARK_OPENAI_BASE}"
      api_key: "${ARK_API_KEY}"
      custom_llm_provider: "openai"
    model_info:
      fallback_order: 3
      fallback_reason: "rate_limit"

  # chat-auto-mini 的 fallback 链 (3层降级)
  # Level 2: New API
  - model_name: chat-auto-mini
    litellm_params:
      model: "gpt-5-mini"
      api_base: http://host.docker.internal:3000/v1
      api_key: "${NEW_API_KEY}"
      custom_llm_provider: "openai"
    model_info:
      fallback_order: 2
      fallback_reason: "rate_limit"

  # Level 3: Volces Ark API (最终降级 - mini)
  - model_name: chat-auto-mini
    litellm_params:
      model: "openai/ark-code-latest"
      api_base: "${ARK_OPENAI_BASE}"
      api_key: "${ARK_API_KEY}"
      custom_llm_provider: "openai"
    model_info:
      fallback_order: 3
      fallback_reason: "rate_limit"

  # ==================================
  # PHYSICAL MODELS - Codex Family
  # ==================================
  # codex-auto: 无降级链 (Volces Ark 不支持 Codex 接口)
  # 仅在虚拟入口定义，无 fallback

  # ==================================
  # PHYSICAL MODELS - Claude Family
  # ==================================

  # claude-auto 的 fallback 链 (2层降级)
  # Level 2: Volces Ark API (最终降级)
  - model_name: claude-auto
    litellm_params:
      model: "anthropic/glm-4.7"
      api_base: "${ARK_CLAUDE_BASE}"
      api_key: "${ARK_API_KEY}"
      custom_llm_provider: "anthropic"
    model_info:
      fallback_order: 2
      fallback_reason: "rate_limit"

  # ==================================
  # CATCH-ALL - 通配符直接透传
  # ==================================
  # Anthropic 模型 (claude-* 和 anthropic/*)
  - model_name: "claude-*"
    litellm_params:
      model: "claude-*"  # 保留 claude 前缀
      api_base: http://host.docker.internal:3000
      api_key: sk-6cjjC0tbmfadXNqsrIABJO6nPBuYXKHtacIU0YFvoRxfTAQh
      custom_llm_provider: "anthropic"

  - model_name: "anthropic/*"
    litellm_params:
      model: "*"  # 保留 anthropic/ 前缀
      api_base: http://host.docker.internal:3000
      api_key: sk-6cjjC0tbmfadXNqsrIABJO6nPBuYXKHtacIU0YFvoRxfTAQh
      custom_llm_provider: "anthropic"

  # OpenAI 模型 (其他所有模型）
  - model_name: "*"
    litellm_params:
      model: "*"
      api_base: http://host.docker.internal:3000/v1
      api_key: sk-6cjjC0tbmfadXNqsrIABJO6nPBuYXKHtacIU0YFvoRxfTAQh

# ==========================================
# 4. ROUTER SETTINGS
# ==========================================
router_settings:
  # 模型组别名 (用于特殊路由场景)
  model_group_alias:
    "chat-auto": "chat-auto"
    "codex-auto": "codex-auto"
    "claude-auto": "claude-auto"

  # Routing strategy - simple-shuffle 会按定义顺序尝试模型
  routing_strategy: "simple-shuffle"

  # Retries and timeout
  num_retries: 3
  timeout: 60

  # 失败后 cooldown
  cooldown_time: 10

# ==========================================
# 5. GENERAL SETTINGS
# ==========================================
general_settings:
  # Master Key for Admin UI authentication AND API requests
  # 客户端统一使用这个 key，LiteLLM 自动转发到配置的后端
  master_key: "${LITELLM_MASTER_KEY}"

  # Admin UI credentials (can also be set via env vars UI_USERNAME/UI_PASSWORD)
  ui_username: "admin"
  ui_password: "admin123"

# ==========================================
# 6. LITELLM SETTINGS
# ==========================================
litellm_settings:
  # Logging
  set_verbose: false
  json_logs: true

  # Drop unsupported params
  drop_params: true

  # Callbacks (for pre_call_hook)
  callbacks: ["vibe_router.router_instance"]

  # Success/failure callbacks
  success_callback: ["vibe_router.router_instance"]
  failure_callback: ["vibe_router.router_instance"]

  # Request timeout
  request_timeout: 600
