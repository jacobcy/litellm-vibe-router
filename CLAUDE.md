# CLAUDE.md

Project documentation for LiteLLM Intelligent Router with 3-tier fallback strategy.

---

## Project Overview

**Architecture**: LiteLLM Proxy + 3-Layer Fallback Strategy  
**Tech Stack**: Python 3.9+, Docker, LiteLLM Proxy, Redis, PostgreSQL  
**Core Feature**: Virtual models with manual selection + automatic fallback on failures

**Current Status**:
- âœ… **6 Virtual Models**: auto-chat, auto-chat-mini, auto-claude, auto-claude-mini, auto-codex, auto-codex-mini
- âœ… **Manual Model Selection**: Users choose standard vs mini variants
- âœ… **3-Layer Fallback**: CLIProxyAPI (OAuth) â†’ New API â†’ Volces Ark
- ðŸš§ **Auto Complexity Routing**: Disabled (Next Plan - needs more testing and real-world data)

### Virtual Models & Fallback Strategy

```
6 Virtual Models (All Configured):
  âœ… auto-chat          (3 layers): Claude Sonnet 4-6 â†’ gpt-5 â†’ glm-4.7
  âœ… auto-chat-mini     (3 layers): Gemini 2.5 Flash â†’ gpt-5-mini â†’ ark-code
  âœ… auto-claude        (2 layers): Claude Sonnet 4-5 â†’ glm-4.7
  âœ… auto-claude-mini   (2 layers): Claude Haiku 4-5 â†’ glm-4.7
  âœ… auto-codex         (1 layer):  gpt-5.2-codex (New API only)
  âœ… auto-codex-mini    (1 layer):  gpt-5.1-codex-mini (New API only)

Fallback Layers:
  L1: CLIProxyAPI (Antigravity OAuth) - Free tier with quota
  L2: New API (è‡ªå»ºè½¬å‘) - Unlimited usage
  L3: Volces Ark (å•†ä¸šä»˜è´¹) - Paid fallback
```

---

## Quick Commands

```bash
# Deployment
./deploy.sh                           # Auto-deploy all services
docker-compose restart litellm        # Restart after config changes

# Testing (setup virtual environment first)
python3 -m venv .venv                 # Create venv (once)
source .venv/bin/activate && pip install requests  # Install deps
python3 tests/test_all_6_models.py    # Quick test all 6 models
source tests/.env && python3 tests/test_route.py   # Full suite
python3 tests/test_remote.py --url http://server:4000 --key sk-xxx

# Manual test
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer ${LITELLM_MASTER_KEY}" \
  -d '{"model": "auto-chat", "messages": [{"role": "user", "content": "hi"}]}'

# Monitoring
docker logs -f litellm-vibe-router 2>&1 | grep VIBE-ROUTER
docker logs litellm-vibe-router 2>&1 | grep "PASSTHROUGH\|Hook triggered"
```

**Admin UI**: `http://localhost:4000/ui/` (admin / from .env) - Generate Virtual Keys here  
**Test Config**: Copy [tests/.env.example](tests/.env.example) to `tests/.env` and add your key

---

## Request Flow

```
Client â†’ LiteLLM (4000) â†’ Virtual Model Selection (Manual)
  â”œâ”€ auto-chat: Standard tasks (claude-sonnet-4-6) with 3-layer fallback
  â”œâ”€ auto-chat-mini: Lightweight tasks (gemini-2.5-flash) with 3-layer fallback
  â”œâ”€ auto-claude: Claude tasks (claude-sonnet-4-5) with 2-layer fallback
  â”œâ”€ auto-claude-mini: Simple Claude (claude-haiku-4-5) with 2-layer fallback
  â”œâ”€ auto-codex: Code tasks (gpt-5.2-codex) New API only
  â””â”€ auto-codex-mini: Simple code (gpt-5.1-codex-mini) New API only
    â†’ Router â†’ L1: CLIProxyAPI (OAuth) â†’ L2: New API â†’ L3: Ark API
```

**Model Selection Guide**:
- **auto-chat**: Complex reasoning, long context, multi-step tasks
- **auto-chat-mini**: Simple queries, quick responses, Q&A
- **auto-claude**: Deep analysis, creative writing, complex reasoning
- **auto-claude-mini**: Simple tasks, quick responses, lightweight
- **auto-codex**: Complex code generation, algorithms, refactoring
- **auto-codex-mini**: Simple code snippets, syntax help, quick fixes

### Fallback Strategy (All Models)

## Key Files

**[vibe_router.py](vibe_router.py)** - Router plugin (metadata tracking only)
- Currently in **passthrough mode** - no automatic routing
- Adds metadata: `routing_mode: manual_selection`, `selected_model`
- `async_pre_call_hook()` - **Returns data unmodified** (L118-160)
- ðŸš§ **Next Plan**: Auto-complexity routing (commented out, L162-210)

**[config_final.yaml](config_final.yaml)** - LiteLLM configuration
- `model_list`: Virtual models + fallback chains with `fallback_order`
- âš ï¸ Use `os.environ/VAR` not `${VAR}` for API keys
- âš ï¸ Must add callbacks in BOTH `general_settings` AND `litellm_settings`

**[docker-compose.yml](docker-compose.yml)** - Services orchestration
- Environment: `PYTHONPATH=/app` + `env_file: .env`
- Volumes: Plugin mounted at `/app/vibe_router.py:ro`

**[.env](/.env)** - Secrets (gitignored)
- `LITELLM_MASTER_KEY`, `UI_USERNAME`, `UI_PASSWORD`
- `CHAT_AUTO_API_KEY` (CLIProxyAPI), `NEW_API_KEY`, `ARK_API_KEY`

---

## Code Style & Configuration

### Python Essentials
- **Type hints**: `def func(x: str) -> bool:`
- **Error handling**: Log errors, return fallback (never `except: pass`)
- **Async hooks**: Must return modified `data` dict

### Critical Patterns
```python
# LiteLLM Hook (MUST return data)
async def async_pre_call_hook(self, ..., data: Dict, ...) -> Optional[Dict]:
    data["model"] = target_model
    return data  # â† Required, or model becomes None
```

```yaml
# Environment Variables (use os.environ/ syntax)
api_key: os.environ/NEW_API_KEY  # âœ… Correct
api_key: "${NEW_API_KEY}"       # âŒ Won't expand

# Callback Registration (BOTH required)
general_settings:
  callbacks: [vibe_router.router_instance]
litellm_settings:
  callbacks: [vibe_router.router_instance]
```

### Virtual Model Configuration
```yaml
# L1: Use actual backend model
- model_name: auto-chat
  litellm_params:
    model: "claude-sonnet-4-6"  # Antigravity model
    api_key: os.environ/CHAT_AUTO_API_KEY

# L2-L3: Same model_name with fallback_order
- model_name: auto-chat
  litellm_params:
    model: "gpt-5"
  model_info:
    fallback_order: 2
```

---

## Troubleshooting

| Issue | Symptom | Fix |
|-------|---------|-----|
| **Hook Not Executing** | Plugin loads but no routing | Add `callbacks: [vibe_router.router_instance]` to `litellm_settings` |
| **Env Vars Not Expanding** | API calls fail with `"${VAR}"` | Use `os.environ/VAR` not `${VAR}` |
| **Model Not Found** | 429/404 from CLIProxyAPI | Use actual backend models (e.g., `gemini-2.5-flash`) |
| **Pydantic Validation** | `model_info.tier` error | Only use `"free"`/`"paid"` or remove tier |

---

## Development

**Add Virtual Model**: Edit `SIMPLE_TASK_TARGETS` â†’ Add 3 layers in `config_final.yaml` â†’ Restart  
**Modify Complexity**: Edit `_calculate_complexity()` or `COMPLEXITY_THRESHOLD = 50`  
**Change Backend**: L1(OAuth)=model names, L2(NewAPI)=keep gpt-5, L3(Ark)=openai/glm-4.7

---

## ðŸš§ Next Plan: Auto Complexity Routing

**Current Status**: Disabled (commented out in [vibe_router.py](vibe_router.py#L162-210))

**Why Disabled**:
- Algorithm needs more testing with real-world data
- Manual selection is more reliable and predictable
- Focus on stable basic service first

**Future Implementation**:
1. Collect real request data and complexity patterns
2. Train/tune prediction model with user feedback
3. Add A/B testing framework to validate routing accuracy
4. Implement gradual rollout with fallback to manual mode

**When to Enable**:
- After gathering â‰¥1000 production requests
- Complexity prediction accuracy >90%
- User satisfaction metrics validated

---

## Quick Reference

| Item | Value |
|------|-------|
| LiteLLM Port | 4000 |
| CLIProxyAPI Port | 8317 |
| Plugin Hook | `async_pre_call_hook` (must return data) |
| Hook Order | Auth â†’ Alias â†’ **Hook** â†’ Router â†’ Backend |
| Environment Syntax | `os.environ/VAR` (not `${VAR}`) |
| Test Command | `python3 tests/test_all_6_models.py` |
| Log Filter | `grep VIBE-ROUTER` or `grep "ROUTING DECISION"` |

**Virtual Models**: `auto-chat`, `auto-chat-mini`, `auto-claude`, `auto-claude-mini`, `auto-codex`, `auto-codex-mini`  
**Backends**: CLIProxyAPI (OAuth) â†’ New API (è‡ªå»º) â†’ Volces Ark (ä»˜è´¹)
