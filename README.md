# LiteLLM Intelligent Virtual Router

## ğŸ¯ Overview

This project implements **intelligent model routing** for LiteLLM Proxy, enabling automatic model selection based on request complexity. Users send requests to virtual models (`chat-auto`, `codex-auto`, `claude-auto`), and the system automatically routes to appropriate physical models based on query complexity and rate limits.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â”€â–¶â”‚  LiteLLM Proxy   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Vibe Router    â”‚â”€â”€â”€â”€â”€â–¶â”‚ New API  â”‚
â”‚             â”‚      â”‚   Port 4000      â”‚      â”‚    Plugin       â”‚      â”‚ Port 3000â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                         â”‚
                              â–¼                         â–¼
                        Virtual Models           Intelligent Routing
                        - chat-auto       â”Œâ”€â”€â–¶ Simple â†’ Mini Models
                        - codex-auto      â”‚    Complex â†’ Main Models
                        - claude-auto     â”œâ”€â”€â–¶ Rate Limit Fallback
                                          â”‚    Wildcard Passthrough
                                          â””â”€â”€â–¶ Direct Model Access
```

## âœ¨ Key Features

- **Intelligent Routing**: Analyzes message complexity to select optimal model
- **Dual-Level Routing**:
  - **Simple queries** â†’ route to lightweight models (cost-effective)
  - **Complex queries** â†’ route to powerful models with rate limit fallback
- **Wildcard Passthrough**: Any model can be requested directly
- **Zero Client Changes**: Use virtual model names, routing is transparent
- **Cost Optimization**: Simple queries use cheaper models automatically
- **Detailed Logging**: Real-time routing decisions visible in logs
- **Admin UI**: Built-in management interface at port 4000

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- Python 3.9+ (for testing)
- New API running on `localhost:3000`

## ğŸš€ Quick Start

### 1. Deploy

```bash
chmod +x deploy.sh QUICKSTART.sh
./deploy.sh
```

This will:
- Start LiteLLM proxy, PostgreSQL, and Redis
- Verify plugin loading
- Check service health

### 2. Test Routing

```bash
# Run comprehensive test suite
python3 test_simple.py
```

### 3. Manual Test

```bash
# Simple query (routes to gpt-5-mini)
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-litellm-master-key-12345678" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "chat-auto",
    "messages": [{"role": "user", "content": "hi"}]
  }'

# Complex query (routes to openai/gpt-5)
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-litellm-master-key-12345678" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-auto",
    "messages": [{
      "role": "user",
      "content": "Implement a distributed algorithm for concurrent data processing"
    }]
  }'

# Direct model access (wildcard passthrough)
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-litellm-master-key-12345678" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "messages": [{"role": "user", "content": "test"}]
  }'
```

## ğŸ“ File Structure

```
liteLLM/
â”œâ”€â”€ vibe_router.py          # Main plugin implementation
â”œâ”€â”€ config_final.yaml       # LiteLLM configuration
â”œâ”€â”€ docker-compose.yml      # Docker deployment (PostgreSQL, Redis, LiteLLM)
â”œâ”€â”€ deploy.sh               # Automated deployment script
â”œâ”€â”€ test_simple.py          # Simple routing tests
â”œâ”€â”€ test_route.py           # Comprehensive test suite
â”œâ”€â”€ test_remote.py          # Remote testing script
â”œâ”€â”€ verify.py              # Verification script
â”œâ”€â”€ QUICKSTART.sh          # Quick start script
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Configuration

### Virtual Models

Three virtual entry points with intelligent routing:

| Virtual Model | Simple Route       | Complex Route         | Rate Limit Fallback |
|--------------|--------------------|-----------------------|-------------------|
| `chat-auto`  | gpt-5-mini        | openai/gpt-5         | gpt-5            |
| `codex-auto` | gpt-5.1-codex-mini| openai/gpt-5.2-codex | gpt-5.2-codex    |
| `claude-auto`| claude-haiku-4-5  | anthropic/claude-sonnet-4-5 | claude-sonnet-4-5 |

### Routing Logic

The plugin calculates a **complexity score** based on:

1. **Message Length**: Longer messages â†’ higher score
2. **Simple Keywords**: `ls`, `cat`, `hi`, `test` â†’ lower score
3. **Complex Keywords**: `implement`, `analyze`, `algorithm`, `design` â†’ higher score
4. **Code Blocks**: Presence of ` ``` ` â†’ higher score
5. **Sentence Count**: Multiple sentences â†’ higher score
6. **Conversation History**: Long threads â†’ higher score

**Decision Threshold**: Score < 50 = Simple, Score â‰¥ 50 = Complex

### Wildcard Passthrough

Any model can be requested directly:

| Pattern      | Provider  | API Base                     | Notes |
|--------------|-----------|-----------------------------|-------|
| `claude-*`   | anthropic| `http://host.docker.internal:3000` | No /v1 |
| `anthropic/*`| anthropic| `http://host.docker.internal:3000` | No /v1 |
| `*`          | openai    | `http://host.docker.internal:3000/v1` | Default |

### Customizing Routes

Edit `vibe_router.py`:

```python
# Simple task targets
SIMPLE_TASK_TARGETS = {
    "chat-auto": "gpt-5-mini",
    "codex-auto": "gpt-5.1-codex-mini",
    "claude-auto": "claude-haiku-4-5"
}

# Complexity threshold (default: 50)
COMPLEXITY_THRESHOLD = 50  # Adjust to tune routing sensitivity
```

### Authentication

**For API Requests:**
```
Authorization: Bearer sk-litellm-master-key-12345678
```

**For Admin UI:**
- URL: `http://localhost:4000/ui/`
- Username: `admin`
- Password: `admin123`

### Environment Variables

Backend API key is configured in `config_final.yaml`:
```yaml
NEW_API_KEY: sk-6cjjC0tbmfadXNqsrIABJO6nPBuYXKHtacIU0YFvoRxfTAQh
```

## ğŸ“Š Monitoring

### View Real-Time Logs

```bash
# All logs
docker logs -f litellm-vibe-router

# Plugin logs only
docker logs -f litellm-vibe-router 2>&1 | grep VIBE-ROUTER

# Routing decisions
docker logs -f litellm-vibe-router 2>&1 | grep "SIMPLE\|COMPLEX"
```

### Check Service Health

```bash
# Health endpoint
curl http://localhost:4000/health

# List models
curl -H "Authorization: Bearer sk-litellm-master-key-12345678" \
  http://localhost:4000/v1/models
```

### Access Admin UI

```bash
# Open in browser
open http://localhost:4000/ui/

# Login with:
# Username: admin
# Password: admin123
```

## ğŸ”¬ Advanced: How It Works

### LiteLLM Request Lifecycle

```
1. Client Request
   â†“
2. Master Key Validation
   â†“
3. async_pre_call_hook â† VIBE ROUTER EXECUTES HERE
   â”‚ - Analyze message complexity
   â”‚ - Simple query: Rewrite model to lightweight (e.g., gpt-5-mini)
   â”‚ - Complex query: Keep model as-is for fallback chain
   â”‚ - Add metadata for observability
   â†“
4. Router Model Resolution (pattern matching, groups)
   â†“
5. Rate Limit Fallback (if needed)
   â”‚ - Try primary model
   â”‚ - If rate limited, try fallback model
   â†“
6. LiteLLM API Call â†’ New API â†’ Actual LLM
```

### Routing Decision Flow

```
Request â†’ chat-auto
    â†“
async_pre_call_hook analyzes complexity
    â†“
    â”œâ”€ Score < 50 (Simple) â†’ Rewrite to gpt-5-mini
    â”‚                           â†“
    â”‚                         Send to backend directly
    â”‚
    â””â”€ Score >= 50 (Complex) â†’ Keep openai/gpt-5
                               â†“
                             Try openai/gpt-5
                                 â†“ (rate limited)
                             Try gpt-5 (fallback)
```

### Plugin Architecture

```python
class VibeIntelligentRouter(CustomLogger):
    SIMPLE_TASK_TARGETS = {
        "chat-auto": "gpt-5-mini",
        "codex-auto": "gpt-5.1-codex-mini",
        "claude-auto": "claude-haiku-4-5"
    }

    async def async_pre_call_hook(self, ...):
        """
        Fires BEFORE router resolution
        Rewrites model for simple queries to lightweight models
        """

        # 1. Calculate complexity
        score = self._calculate_complexity(messages)

        # 2. Select target for simple queries
        if score < 50:
            target_model = self.SIMPLE_TASK_TARGETS[original_model]
            data["model"] = target_model
            # Simple queries skip fallback chain

        # 3. CRITICAL: Return modified data
        return data
```

## ğŸ› Troubleshooting

### Plugin Not Loading

**Symptoms**: No `[VIBE-ROUTER]` logs, 400 errors

**Fix**:
```bash
# Check plugin logs
docker logs litellm-vibe-router 2>&1 | grep VIBE-ROUTER

# Verify PYTHONPATH
docker exec litellm-vibe-router env | grep PYTHONPATH

# Test plugin import
docker exec litellm-vibe-router python3 -c "import vibe_router; print('OK')"
```

### Admin UI Not Connecting to DB

**Symptoms**: "Authentication Error, Not connected to DB!"

**Fix**:
```bash
# Check PostgreSQL status
docker ps | grep postgres

# Verify DATABASE_URL
docker exec litellm-vibe-router env | grep DATABASE_URL

# Restart services
docker-compose down
docker-compose up -d
```

### Model Not Found Errors

**Symptoms**: `ProxyModelNotFoundError: model=chat-auto`

**Cause**: Virtual models not defined in `config_final.yaml`

**Fix**: Ensure virtual models exist in `model_list` with fallback chain:
```yaml
model_list:
  # Virtual entry point
  - model_name: chat-auto
    litellm_params:
      model: "openai/gpt-5"

  # Rate limit fallback
  - model_name: chat-auto
    litellm_params:
      model: "gpt-5"
    model_info:
      fallback_order: 2
      fallback_reason: "rate_limit"
```

### Connection Errors

**Symptoms**: `Connection refused` to New API

**Fix**:
```bash
# Check New API is running
curl http://localhost:3000/v1/models

# Verify docker host gateway
docker exec litellm-vibe-router ping -c 1 host.docker.internal
```

## ğŸ“ License

MIT

## ğŸ“ References

- [LiteLLM Docs](https://docs.litellm.ai/)
- [Custom Callbacks](https://docs.litellm.ai/docs/proxy/custom_callbacks)
- [Admin UI](https://docs.litellm.ai/docs/proxy/admin_ui)
